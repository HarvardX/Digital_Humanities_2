{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Text - Preparing the ECCO Corpus for Analysis - Complete Notebook\n",
    "\n",
    "This notebook will introduce you to a number of the steps that you are likely to take when you are preparing text for analysis. As Cole points out in the video, our main objective will be to transform human-readable text into something that your computer can make sense of. Broadly, the steps that we'll cover here will fall into two buckets. Some, like lowercasing and the removal of punctuation, will affect single character. Others, like the removal of stopwords, stemming, and lemmatization, will affect whole token or words.\n",
    "\n",
    "Which processes you choose to run are very much dependant on what you want to do with your text. If you want to count every word in a document, for instance, you certainly wouldn't want to remove common words like \"the,\" \"and,\" \"in\", and \"of\". Similarly, we'll soon take a look at Named Entity Recognition, a process that often relies on the existence of capitalized letters in your text. The point is this: there's no one sequence of processing steps that you can be sure to use for every analytic technique. These sorts of preparatory steps can seem rather simple, but you need to be attentive. Making the wrong choice can have dire consequences for the success of your analysis.\n",
    "\n",
    "That said, with a bit of experience and careful planning, you're unlikely to go astray. For the most part, these techniques are relatively easy to master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First Steps\n",
    "\n",
    "As Cole says in the accompanying video for this notebook, we'll start by looking at Daniel Defoe's <i>Robinson Crusoe</i>. <b>Be careful though, because for this first section of the notebook, we're going to do something slightly different from what Cole does in the video.</b>\n",
    "\n",
    "Cole starts by using [os](https://docs.python.org/3/library/os.html) and [Path](https://docs.python.org/3/library/pathlib.html) to move around his computer. He had to seek out his `.txt` file of <i>Robinson Crusoe</i>. We've made things a bit easier of you, however, because you should find that this file exists in the same directory as this notebook. Let's make sure, though, that we're in the right directory, and that this contains Defoe's novel.\n",
    "\n",
    "First, download the `os` and `Path` modules and check your current working directory using `os.getcwd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import [glob](https://docs.python.org/3/library/glob.html). Rather unfortunately, `glob` doesn't stand for anything. It's a module that lets you check the pathnames or filenames that are contained in a directory. As in the cell below, you can use the `glob.glob()` method to take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of filenames\n",
    "import glob\n",
    "print(glob.glob(\"*.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Darn. Nothing there. We've intentionally been a bit tricky, and placed the our text file a bit further away, so that you have to open it for yourself. Start by defining your home directory and using the CWD pathname above to construct your own `textdirectory` variable. In this case, although we're currently in the `dh2/sec3` folder, let's just make `/dh2/coprora_and_metadata` our text directory. It should look something like this, but you might have to modify it, depending on where you've placed our course files on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdirectory = home + '/dh2/corpora_and_metadata/'\n",
    "\n",
    "os.chdir(textdirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check again to see if our novel by Defoe is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glob.glob(\"*.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found it! Now that we've located our text file of the novel, we're ready to open it and to follow along with Cole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open first file\n",
    "doc1 = open(textdirectory + \"crusoe.txt\", \"r\")\n",
    "\n",
    "# Read the document and print its contents\n",
    "doc1Txt = doc1.read()\n",
    "print(doc1Txt[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examining the Text\n",
    "\n",
    "You can run the following cell to load the title page of Robinson Crusoe right into your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='crusoe_title_page.jpg', width = 1000, height = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look closely at this image. What do you see?\n",
    "\n",
    "Just as we saw when we first examined the ECCO dataset, many of the elements of the physical book cannot be rendered in a `.txt` file. That's obviously true for the frontispiece, but you may also notice that eighteenth-century spelling is normalized (\"SURPRIZING\" becomes \"SURPRISING\" and \"PYRATES\" becomes \"PIRATES\"). For the purposes of this course, we're working with a set of documents whose spelling has been \"normalized\" or standardized. This moves us further away form the historical book, but it also means that our computers will have a far easier time recognizing when the same word is used, from one context to another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Character-Based Transformations: Lowercasing and the Removal of Punctuation\n",
    "\n",
    "Cole points out that we will soon tokenize the text, to turn our words into coherent tokens. Before we do that, though, we need to apply a number of important processing steps to the letters themselves.\n",
    "\n",
    "First, let's lowercase every character in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = doc1Txt.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (txt[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That done, let's strip out all the punctuation, which we need to import from the `string` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move forward, let's take a look at what it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(punctuation[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a string. That's interesting. If `punctuation` is a string rather than a module, we can't apply it to our text directly, as we did with `lower()`. Instead, we'll have to be a bit more clever. Can you think of a solution?\n",
    "\n",
    "### Pause the Video Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've thought about how to apply punctuation to your text, restart the video, and Cole will provide a brief explanation. (Yes, the solution appears directly below!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''.join(c for c in txt if c not in punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a slightly longer snippet of the text and examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (txt[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good. You may, however, want to do spot checks by looking at other parts of the document, changing the text window to something like `[410000:420000]`, for instance. Just to be safe, let's add a few characters that our `punctuation` string omits. If you examine the cell below carefully and compare it to the original string, you'll see that the quotation marks we are adding are differently formatted. This can make a big difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation += \"“”‘’↩\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''.join(c for c in txt if c not in punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (txt[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. From Characters to Words: Tokenization and Stop Words\n",
    "\n",
    "Let's first get [punkt](https://www.nltk.org/_modules/nltk/tokenize/punkt.html), which is contained in the supremely useful Natural Language Toolkit [nltk](https://www.nltk.org/) module. We'll be using NLTK throughout this course, so make sure to take a look at the documentation. Do be aware - it's pretty big, as far as modules go, so you can expect that it will take a few seconds to import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want all of nltk, so we'll use this technique to download `punkt`, rather than the usual \"from nltk import punkt\", as we've done before.\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Now, let's the text into individual words\n",
    "words = nltk.tokenize.word_tokenize(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as always, let's examine what we get as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've transformed our entire novel - the whole `string`, if you will - into a list. Let's take a moment to confirm that's the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list that we now have still includes words like <b>_the_</b>, and <b>_and_</b> and <b>_of_</b> - the stop words we may want to remove. Again, as with many of these techniques, whether we want to remove stop words or not depends a great deal on what we want sort of analysis we want to perform. Often, you'll want to remove stop words if they might add noise to your results, or swamp form semantically meaningful words.\n",
    "\n",
    "For now, let's get rid of them. The technique that we use here is similar to how we removed punctuation. In thise case, though, we're working with lists, not strings. Each word that's not considered a stopword will be joined together in a new string that we're calling `usefulTxt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a set of common stopwords from NLTK\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "# remove stopwords from the text\n",
    "usefulTxt = ' '.join([word for word in txt.split() if word not in (stopwords.words('english'))])\n",
    "print(usefulTxt[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you read through the words that are left, you might think that some of these words should be removed, even though they weren't - adverbs, ordinal numbers, and single letters are especially good candidates: _eight_, _near_, _wherein_, _w_, _ever_, _one_.\n",
    "\n",
    "Let's take a quick look at the list of stop words that comes out of the NTLK box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew. That's only 179 items. It might seem like a lot, especially since these words are so common, but it may well be the case that we'll have to add to this list later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Repeat the Processing Steps in a Single Cell\n",
    "\n",
    "As Cole points out in the video, it's rather cumbersome to run all of these steps singly, one cell after another. It slows down what could be an otherwise very elegant and efficent process. This is important, because we use these processing techniques so often in the digital humanities.\n",
    "\n",
    "Take a moment, and give it a shot. Start with `doc1Txt`, and do the following operations: `lowercase`, `remove punctuation`, `tokenize`, `remove stopwords`, and `join the tokens together again`. With all of that complete, print the first 500 items in the resulting string. You should be able to do all of this in just a few lines of code. Pause the video and give it a shot.\n",
    "\n",
    "#### Pause the video and write a three-line script in a single cell that processes doc1Txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you get it? <b>Cole says that you should get the same results as when we removed stop words, but that isn't strictly right, because we've rejoined the resulting words into a single string.</b> It isn't common to rejoin our tokens, once stop words are removed, but it might be useful if, for instance, you were to want to save the resulting string as a new file, so that you can return to it later. In the context of a single Notebook, however, you wouldn't normally rejoin your list of useful words, once you have them.\n",
    "\n",
    "Now that the video is complete, carry on with the rest of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understand the mechanics of the processing steps above, you'll find it fairly similar to apply stemming and lemmatization. These two techniques can look quite similar to each other, but it's important to remember how they are different. Let's start with stemming. Stemming is a process that generally tries to reduce inflection, but it's rather brutal. It tends to chop off the inflected ending of a word, leaving you with a truncated form -- a <i>stem</i> -- which might not even be a recongizeable word. It's good to be aware that this can change all sorts of different types of speech. \n",
    "\n",
    "Let's import [PorterStemmer](https://www.nltk.org/howto/stem.html) from NLTK and test it against a simple list of inflected words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "light_list = [\"light\", \"lit\", \"lighted\", \"lights\", \"lighting\", \"lighter\", \"lightly\"]\n",
    "\n",
    "light_stems = [ps.stem(word) for word in light_list]\n",
    "print(light_stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. This should give us good reason to be cautious when we use this technqiue. At first glance, it might feel inuitively right that the noun `lighter`, something you use to start a fire, is different from the more abstract `light`, but what it were used as a comparative adjective? Should the adverb `lightly` necessarily be treated as a different word? It's important to see that stemming is a rather blunt technique, and when we use it, we're relinquishing a lot of control.\n",
    "\n",
    "That said, let's try applying PorterStemmer to <i>Robinson Crusoe</i>. First, we'll produce a list of the \"useful\" words in the novel, with stopwords removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usefulWordList = stems = [word for word in usefulTxt.split()]\n",
    "print(usefulWordList[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's a simple matter to reduce each word to its stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = [ps.stem(word) for word in usefulWordList]\n",
    "print(stems[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. Here you can really see what we mean, when we say that stemming is \"brutal.\" The ends of many of the words are just lopped off, and there isn't any consideration of context. Even \"crusoe,\" a name, is reduced to \"cruso.\"\n",
    "\n",
    "Lemmatization is perhaps a more subtle technique, which attempts not just to reduce inflection, but to return a word to its root. Where it can't achieve this, it tends to leave the word as it is. Let's test it out on the same variables. In this case, we'll use NLTK's inbuilt [WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "light_lemmas = [wordnet_lemmatizer.lemmatize(word) for word in light_list]\n",
    "print(light_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is a sort of minimally invasive technique. If you compare the lemmas to the original list of \"light\" terms, you'll see that \"lights\" is the only word that was changed.\n",
    "\n",
    "Let's now try it against Defoe's novel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [wordnet_lemmatizer.lemmatize(word) for word in usefulWordList]\n",
    "print(lemmas[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the difference between the original text and the stems and lemmas we just produced. We'll start with token 82, because this is where the preface of the novel begins in earnest, after the noun-heavy title page. In this case, we'll draw up a simple chart with three columns, so that we can compare the original word with its stem and lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortlist = usefulWordList[82:132]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Stem\",\"Lemma\"))\n",
    "for word in shortlist:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,ps.stem(word),wordnet_lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. What we can see, then, is that a our lemmatizer is much more \"gentle\" on our text. That's likely to cause us to prefer it, in most cases, but again, this will involve a degree of judgment, and it will very much depend on what we want to do with our text, once we've transformed it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using Loops to Process a Whole Dataset\n",
    "\n",
    "Now that you've applied those transformations to a single document, we're going to write a loop in a single cell that applies a couple of minimal transformations to our entire working corpus. We don't want to do too much, because we can't yet anticipate how we'll use this data. Let's move over to our `corpora_and_metadata` directory, and create a new folder called `working_set_cleaned`, where we'll place the cleaned files, once they're ready. Then let's move over to `working_set`, which contains the original enriched ECCO-TCP working set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora = home + '/dh2/corpora_and_metadata/'\n",
    "\n",
    "os.chdir(corpora)\n",
    "\n",
    "if not os.path.exists('working_set_cleaned'):\n",
    "    os.makedirs('working_set_cleaned')\n",
    "    \n",
    "os.chdir(corpora + 'working_set')\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now, create a list of all the filenames in your `working_set` directory (remember, these include the volumes that Stephen added to the ECCO-TCP). Once you've done that, write a loop that iterates over each filename, removes its punctuation and lowercases it, and then prints it to a new file in `working_set_cleaned`. We don't want to do anything else, yet, because at the beginning of the Simple Counting lesson we'll count the number of raw words in each file.  Writing the loop to do this processing can be a bit tricky, so don't worry if you don't get it right away. The most difficult part involves opening a clean file in the new \"clean\" directory and writing the processed text to that file, along with its title, so we've gone ahead and done that for you. \n",
    "\n",
    "<b>Remember. If you get stuck, you can always refer to the \"Complete\" version of this notebook.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedtextdirectory = corpora + 'working_set_cleaned/'\n",
    "\n",
    "filenames =       #Get a list of the filenames in the `working_set` directory\n",
    "\n",
    "for i in filenames:\n",
    "    objName = open(i, \"r\")\n",
    "    readObjName = objName.read()\n",
    "    txt =         #Remove punctuation and lowercase each character, then them into a single string\n",
    "    txt = regex.sub(\" \", txt)\n",
    "    file = open(cleanedtextdirectory + str(i),\"w\")\n",
    "    file.write(txt)\n",
    "    file.close()\n",
    "print(\"Done! Check out \" + cleanedtextdirectory + \" on your hard drive to see the results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way to go! You've now learned the basics of preprocessing text, so that it's ready for analysis. Go on to the next page in the course, we'll review the progress we've made than then address cleaning metadata. Once you have these skills under your belt, you'll be well-prepared to embark on text analysis. We're almost there!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
