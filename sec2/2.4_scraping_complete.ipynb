{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping - Collecting Data about Monsters - Complete Notebook\n",
    "\n",
    "## 1. First Steps: Using Beautiful Soup to Scrape a Wikipedia Page\n",
    "\n",
    "We are going to scrape Wikipedia for information about various mythological creatures. The information we want exists in multiple layers, like a tree structure. We're starting from [this list](https://en.wikipedia.org/wiki/Lists_of_legendary_creatures). This is a top level page which is a list of lists, one for each letter (eg `List of legendary creatures (A)`). The `A` sublist contains a list of creatures, as well as each creature's cultural origin. We can go one level deeper to the creature itself for even more information.\n",
    "- Lists of legendary creatures\n",
    "    - List of legendary creatures (A)\n",
    "    - List of legendary creatures (B)\n",
    "        - Ba (Egyptian)\n",
    "        - Baba Yaga (Slavic)\n",
    "        - Backoo (Guyanese) ...\n",
    "    - List of legendary creatures (C) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the list of lists and try to extract our sublists. We'll use the Requests library, as we did with APIs, as well as [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#), which is a library for extracting data from HTML and XML. We're going to use the `html5lib` parser with Beautiful Soup because it is pretty lenient and creates valid HTML; the downside is that it is slightly slower than some other parsers.\n",
    "\n",
    "So, first, let's import all the modules we need, including Beautiful Soup, which we'll simply call `bs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup as bs\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our modules, the following cell will scrape our list of lists of legendary creatures.\n",
    "\n",
    "As with our API queries, the line of script that uses `requests` to do the scraping is very straightforward: `R = requests.get(list_of_lists_url)`.\n",
    "\n",
    "Once we have that HTML content, we'll then parse it into a tree structure of Python objects using Beautiful Soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_lists_url = \"https://en.wikipedia.org/wiki/Lists_of_legendary_creatures\"\n",
    "R = requests.get(list_of_lists_url)\n",
    "soup = bs(R.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the massive blob of HTML that we've just scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a little unweildly. Let's instead see a \"pretty\" version of the HTML tree using the `soup.prettify()` method. Once you run the following cell, the HTML will be structured in a much more human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, instead, we print the actual raw content using `print(R.content)`, we can see that that it looks like one long string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(R.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we downloaded without any formatting. It's actually a sequence of <b>byte literals</b>. You can tell this because byte literals are prefixed by `b` when printed.\n",
    "\n",
    "So far, so good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Navigating HTML with Beautiful Soup\n",
    "\n",
    "There's a lot that you can do with the HTML data that you've just scraped, but to do so you have to navigate this mass of information effectively. Fortunately, Beautiful Soup includes a number of methods that are built for just this purpose. Most of these methods isolate specific HTML tags, like `<title>` or `<body>` or `<p>`. We'll look at a few here, but you can always [consult the documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) if you want to see a full list.\n",
    "\n",
    "First of all, we can isolate the text from the page without any HTML markup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a little messy at the top of the page, but if you scroll down, you'll see that a good deal of the text is captured quite cleanly.\n",
    "\n",
    "Similarly, you can isolate the `title` element. By using the `title` method by itself, you'll retain the HTML tags for this element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if you prefer, you can remove those tags by isolating the string itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we get the first paragraph element, using `.p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to modify the cell above to locate other common HTML elements, like `.head`, `.body`, `.div` and `.a`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, you'll want to search for a specific occurence of a tag using attributes such as a classname or id. You can use the `.find()` method to specify these attributes. Let's grab the Table of Contents from our soup. In a browser window, open up [the Wikipedia page]('https://en.wikipedia.org/wiki/Lists_of_legendary_creatures'). Right click on the table of contents and choose \"Inspect\" (in Chrome) or \"Inspect Element\" (Firefox). This will show the current state of the DOM and open to the point in the DOM or \"Document Object Model\" where you've clicked. You should see that there is a `<div>` with an `id` of `\"toc\"`. Let's select that with BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.find(\"div\", id=\"toc\")\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It still looks a bit complicated, but we've cleanly isolated the Table of Contents for this particular page.\n",
    "\n",
    "Now that we've isolated this one `div` tag, we can see all sorts of information about it, including its type, name, attributes, and id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tag))\n",
    "print(tag.name)\n",
    "print(tag.attrs) \n",
    "print(tag['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have just used `soup.find(id=\"toc\")` because HTML IDs should be unique, but sometimes you'll want to combine selector attributes as we did here. This shows that BeautifulSoup tags are their own Python class, and have names and attributes. Those attributes can be accessed just like you would access a dictionary property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compiling the Alphabetical Lists of Legendary Creatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen, in order to compile a master list of legendary creatures, we'll have to scrape each of the alphabetical lists sequentially: all the creatures that start with \"A\" (like, \"Anubis\"), all that start with \"B\" (like, \"Basilisk\"), etc. To do that, we'll need the corresponding URL for each of the alphebetical lists.\n",
    "\n",
    "Let's start by trying to get every instance of the `a` or `anchor` tag, which marks each link on our page. To do this, we'll use the `.find_all()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup.find_all('a')\n",
    "print(len(all_links))\n",
    "all_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, that's way too many links! <b>(You'll probably see a different number from what Cole sees in the video.)</b> We need to find just the hyperlinks for the alphabetical lists. Go ahead and try finding one of the links in the developer's console and seeing if there are any special attributes that might allow us to easily select it.\n",
    "\n",
    "To make it easy for you, we've included this little snippet of our pretified HTML, so you can see the beginning of the list of links that concerns us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Prettified HTML](alphabetical_lists_-_legendary_creatures.png \"Alphabetical Lists - Legendary Creatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it looks like there's no class for the links we want, nor is there an id or class on the containing list element (`<ul>`). There is an ID in the span of the header above the list element we want (`Alphabetical_lists`). We'll need to do some slightly finicky traversing of the tree to get the data: first, find the span; then go from the span to the header (`parent`); get the first member of the parent's `siblings`; and finally, get all of the links in that unordered list (`<li>` tags in the `<ul>`). You can do this using `.contents`, which returns a list, or `.children`, which returns an iterator.\n",
    "\n",
    "Let's find the `span` with the `id` \"Alphabetical_lists` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_header = soup.find(id=\"Alphabetical_lists\")\n",
    "print(tag_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, navigating through this soup of HTML can be a little tricky, but beautifulsoup fortunately has a number of methods that can be used for just this purpose:\n",
    "\n",
    "    `parent` - moves from one element in the HTML to another that contains it\n",
    "    `children` - which does the opposite, moves from an an element to those that it contains\n",
    "    `next_sibling` - moves between two elements on the same level\n",
    "    \n",
    "Follow along as Cole works his way through the HTML to find the elements we need.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_parent = tag_header.parent\n",
    "print(tag_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_nextsib = tag_parent.next_sibling\n",
    "print(tag_nextsib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Cole points out, `next_sibling` will sometimes give you empty blank space. Instead, use `find_next_sibling` to proceed right to the unordered list that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_ul = tag_parent.find_next_sibling()\n",
    "print(tag_ul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one last step. Now that we have this list, we have to be able to isolate each subdirectory, first by singling out one HTML link, and then using `.get(\"href\")` to extract the subdirectory URL. It works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link1 = tag_ul.find(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link1.get(\"href\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've isolated both our unordered list and the subdirectory URL within each link, you should be able to use what you know to produce a list of each of the complete alphabetical URLs.\n",
    "\n",
    "On your own, take a stab at writing a loop that 1) singles out the list element for each letter, 2) identifies each URL subdirectory, 3) appends that subdirectory to the main wikipedia URL , which is `\"https://en.wikipedia.org\"`, and 4) finally, make a list of each of these complete URLs.\n",
    "\n",
    "Here, we've given you a head start with some hints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs = []\n",
    "\n",
    "for link in tag_ul.find_all('a'):\n",
    "    url = \"https://en.wikipedia.org\" + link.get('href')\n",
    "    hrefs.append(url)\n",
    "print(hrefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pause the video here and complete the loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scraping List Pages and Creating a Dictionary of Creatures\n",
    "\n",
    "At this point, you've learned how to scrape a webpage and parse its HTML to find the elements that you need. That was our main objective. For the sake of completeness, we'll show you how Cole went on to scrape a much more complete database, with all sorts of information about these legendary creatures. Much of this is pretty finicky, so we're going to move much more quickly, and we won't be walking through each step in the process.\n",
    "\n",
    "First, we're going to compile all the creatures into one source. Let's make a dictionary for that, using creature names as keys. Then we'll want to create a function that we can reuse to scrape each of the list pages for the creatures. There are several pages that have some irregular items without links or the dashes we're using to separate the creature names from their short descriptions.\n",
    "\n",
    "Here's the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_creatures = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the function Cole wrote to populate his dictionary. If this looks complicated, much of the code here is involved in cleaning up the names and short descriptions for each creature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_list_page(url):\n",
    "    \"\"\"\n",
    "    Scrapes a list of creature pages.\n",
    "    Returns a dictionary of creatures with names, titles (of links), links, short descriptions, and cultures.\n",
    "    \"\"\"\n",
    "    \n",
    "    creatures = {}\n",
    "    R = requests.get(url)\n",
    "    soup = bs(R.content, 'html5lib')\n",
    "    list_items = soup.find(\"div\", class_=\"mw-parser-output\").find_all(\"ul\")[1].find_all(\"li\")\n",
    "    for li in list_items:\n",
    "        if len(li.find_all(\"a\")) < 1: # There are a couple items without links\n",
    "            split_text = li.get_text().split(\"-\", 1)[0].strip()\n",
    "            name = split_text[0].strip()\n",
    "            desc = split_text[1].strip()\n",
    "            creatures[name] = {\n",
    "                \"name\": name,\n",
    "                \"short_description\": desc\n",
    "            }\n",
    "        else:\n",
    "            name = li.a.contents[0]\n",
    "            creatures[name] = {}\n",
    "            creatures[name][\"name\"] = name\n",
    "            creatures[name][\"title\"] = li.a['title']\n",
    "            creatures[name][\"link\"] = requests.compat.urljoin(\"https://en.wikipedia.org/wiki\", li.a['href'])\n",
    "            if \"-\" in li.get_text(): # There are a couple items where the dash isn't surrounded by spaces\n",
    "                creatures[name][\"short_description\"] = li.get_text().split(\"-\", 1)[1].strip()\n",
    "            if(len(li.find_all(\"a\")) > 1): # Couple of items without links\n",
    "                creatures[name][\"culture\"] = li.find_all(\"a\")[1].contents[0]\n",
    "                creatures[name][\"culture_title\"] = li.find_all(\"a\")[1]['title']\n",
    "                creatures[name][\"culture_href\"] = requests.compat.urljoin(\"https://en.wikipedia.org/wiki\", li.find_all(\"a\")[1]['href'])\n",
    "    return creatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this on one page of our `hrefs` list to make sure it works. If that looks good, then we can run it on all the subpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hrefs[0])\n",
    "all_creatures.update(scrape_list_page(hrefs[0]))\n",
    "all_creatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful. Now that we have that information, let's run a loop that collects data from each of the alphabetical URLs we collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for creature_list_url in hrefs:\n",
    "    creatures = scrape_list_page(creature_list_url)\n",
    "    all_creatures.update(creatures)\n",
    "print(len(all_creatures.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our complete list of creatures, it's a simple matter to print the information for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_creatures[\"Vampire\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Our Creatures\n",
    "\n",
    "Great! That's a lot of data. Let's export it all to a CSV in case we want to import it again later. We could use the `csv` library, but Pandas has a quick handy function for this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "creatures_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, creature in all_creatures.items():\n",
    "    creatures_list.append(creature)\n",
    "df = pd.DataFrame(creatures_list)\n",
    "df.to_csv(\"all_creatures.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've made our CSV and the job is done! Find the csv in your section 2 directory and check in out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding More Data\n",
    "There's also some data we'd like to get from individual creature pages, like a more complete description; image links; and \"See Also\" links to other related pages. We'll need to be fairly flexible about the function we write to handle this scraping, because individual creature pages will be more heterogenous than the list pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_before_this(tag):\n",
    "    \"\"\"Find the first tag we want to stop searching at\"\"\"\n",
    "\n",
    "    if tag.name == \"h2\":\n",
    "        return True\n",
    "    elif tag.attrs.get(\"class\") == \"toc\":\n",
    "        return True\n",
    "    elif tag.attrs.get(\"id\") == \"See_also\":\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_creature(url, name=\"\", download=False):\n",
    "    \"\"\"Scrape an individual creature page\"\"\"\n",
    "    \n",
    "    R = requests.get(url)\n",
    "    soup = bs(R.content, 'html5lib')\n",
    "    creature = {}\n",
    "    \n",
    "    # Get paragraphs from start to stop_at: TOC / References / See Also\n",
    "    stop_at = soup.find(skip_before_this)\n",
    "    paragraphs = stop_at.find_all_previous(\"p\")\n",
    "    desc_list = []\n",
    "    for p in paragraphs:\n",
    "        desc_list.append(re.sub('\\[[0-9]\\]','', p.get_text().strip()))\n",
    "    desc_list.reverse()\n",
    "    desc = \" \".join(desc_list)\n",
    "    desc = desc.replace(\"\\r\",\" \")\n",
    "    desc = desc.replace(\"\\n\",\" \")\n",
    "    creature[\"long_description\"] = desc\n",
    "    \n",
    "    # Get See Also if exists - UL after id=\"See_also\"\n",
    "    if(soup.find(id=\"See_also\")):\n",
    "        try:\n",
    "            see_also_bs = soup.find(id=\"See_also\").parent.find_next_sibling(\"ul\")\n",
    "            if see_also_bs is not None:\n",
    "                lis = see_also_bs.find_all(\"li\")\n",
    "                see_also = []\n",
    "                if(len(lis) > 0):\n",
    "                    for li in lis:\n",
    "                        see_also.append(li)\n",
    "                    creature[\"see_also\"] = see_also\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}\")\n",
    "            print(soup.find(id=\"See_also\").parent.find_next_sibling(\"ul\"))\n",
    "            \n",
    "    # Get pictures if they exist\n",
    "    image_urls = []\n",
    "    images = soup.find_all(\"img\", class_=\"thumbimage\")\n",
    "    for img in images:\n",
    "        clean = \"https://\" + img['src'].strip(\"//\")\n",
    "        image_urls.append(clean)\n",
    "    if len(image_urls) > 0:\n",
    "        creature[\"images\"] = image_urls\n",
    "        \n",
    "    if(download):\n",
    "        part = url.split(\"/\")[-1]\n",
    "        filename = f\"creature_pages/creatures/{part}.html\"\n",
    "        all_creatures[name]['localfile'] = filename\n",
    "        with open(filename, mode=\"wb\") as file:\n",
    "            file.write(R.content)\n",
    "        \n",
    "    return creature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on a single creature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrape_creature(all_creatures[\"Angel\"][\"link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all lists\n",
    "for url in hrefs:\n",
    "    part = url.split(\"/\")[-1]\n",
    "    filename = f\"{part}.html\"\n",
    "    R = requests.get(url)\n",
    "    with open(filename, mode=\"wb\") as file:\n",
    "        file.write(R.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape All the Creatures\n",
    "And now let's run it on the full list. Be warned, this cell could take a while, probably about half an hour!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key, creature in all_creatures.items():\n",
    "    if creature.get(\"long_description\") is None and creature.get(\"images\") is None:\n",
    "        try:\n",
    "            creature_data = scrape_creature(creature['link'], name=creature['name'], download=True)\n",
    "            all_creatures[creature['name']].update(creature_data)\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_creatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Our Data\n",
    "Great! That's a lot of data. Let's export it all to a CSV in case we want to import it again later. We could use the `csv` library, but Pandas has a quick handy function for this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "creatures_list = []\n",
    "for key, creature in all_creatures.items():\n",
    "    creatures_list.append(creature)\n",
    "df = pd.DataFrame(creatures_list)\n",
    "df.to_csv(\"all_creatures_updated.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to reimport the data from a CSV, we need to read it and assign it to `all_creatures`. `csv.DictReader` creates OrderedDicts, so we'll cast it to a regular Dictionary. We'll also make sure that each creature's `images` are an actual list, not just a string representation of a list, by using the `ast` library to evaluate the list correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimport from CSV - only run if you want to (re)load the data\n",
    "import csv, ast\n",
    "input_file = csv.DictReader(open(\"all_creatures_updated.csv\"))\n",
    "all_creatures_new = {}\n",
    "for row in input_file:\n",
    "    name = row['name']\n",
    "    all_creatures_new[name] = dict(row) #casting from OrderedDict to a regular Dictionary\n",
    "    if row['images']:\n",
    "        all_creatures[name]['images'] = ast.literal_eval(all_creatures[name]['images']) #Evaluate the list using ast library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what can we do with this data? We can display any creature quite easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import display, Image, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_creature(name):\n",
    "    \"\"\" Display a creature from all_creatures, given an exact matching name \"\"\"\n",
    "    try:\n",
    "        creature = all_creatures[name]\n",
    "        htmlOutput = '<style> .boxes { width: 25%; float: left } </style>'\n",
    "        htmlOutput += f\"<h2>{name}</h2><div><ul>\"\n",
    "        for key, value in creature.items():\n",
    "            if key != \"images\" and key != \"see_also\":\n",
    "                htmlOutput += f\"<li>{key}: {value}</li>\"\n",
    "        htmlOutput += \"</ul>\"\n",
    "        images = creature.get(\"images\")\n",
    "        if images is not None:\n",
    "            for imageurl in images:\n",
    "                htmlOutput += f\"<div class='boxes'><img src='{imageurl}' style='max-height:200px; max-width:200px;'></div>\"\n",
    "        htmlOutput += \"</div>\"\n",
    "        display(HTML(htmlOutput))\n",
    "    except KeyError as e:\n",
    "        print(\"No legendary creature by that name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_creature(\"Basilisk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also start searching our data for creatures from specific cultures, perhaps with a keyword as well, and then displaying all our results:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
