{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Concept Search to the Eighteenth-Century Dataset - Complete Notebook\n",
    "\n",
    "Finally, we've reached the last stage of our project, where we put together everything we've learned to search for concepts across our entire dataset. Back in the simple counting notebook, we counted up single tokens - importantly, the token \"principle\" - in a blunt, one-word appropximation of the technique we'll use here. Concept search involves three key advances over other types of search:\n",
    "\n",
    " 1. Many words are included in a search instead of one, so that we retrieve texts based on how well they represent an idea, rather than just a single token;\n",
    " 2. Document scores are derived from TF-IDF statistics, so that the relevance of rare and common words is balanced out;\n",
    " 3. Volumes are broken into smaller chunks of 1000 words, so that we identify short passages inside a text that discuss the passage in question. This has the added benefit of giving us much more precise results.\n",
    "\n",
    "As we've said, there are a number of ways to start this sort of analysis, from using the results from topic modelling or word embeddings, to a more hands-on technique, which involves feeding in a set of loading passage, and evaluating them for their most frequent words.\n",
    "\n",
    "## 1. Build a Query from Your Vector-Model Results\n",
    "\n",
    "Let's start with the most simplest way to build a search, which is simply to us the cluster of terms nearest to `principle` that we derived in the previous lesson. Make sure to go through and remove any words that you would prefer not to include. You should keep in mind, too, that this first run may not be the most effective demonstration of the possibilities of concept search, because most of the words in this list are near cognates, rather than words that don't necessarily mean the same thing as our concept of interest, but that do appear alongside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ['principles', 'system', 'supposition', 'motive', 'conviction', 'proposition', 'concept', 'doctrine', 'basis', 'morality', 'reasoning', 'notion', 'criterion', 'precept', 'unity', 'theory']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to run all our usual cleaning steps on the string, so that it accords with any tokens found in the texts that are searched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation += \"“”‘’↩\"\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "more_stopwords = (stopwords.words('english')) + [\"0\", \"1\", \"10\", \"100\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"a\", \"able\", \"adam\", \"also\", \"also\", \"although\", \"among\", \"another\", \"away\", \"b\", \"began\", \"c\", \"came\", \"could\", \"d\", \"de\", \"done\", \"e\", \"eight\", \"et\", \"even\", \"even\", \"ever\", \"every\", \"every\", \"f\", \"first\", \"five\", \"found\", \"four\", \"g\", \"gave\", \"give\", \"go\", \"good\", \"great\", \"h\", \"high\", \"however\", \"i\", \"ii\", \"iii\", \"indeed\", \"j\", \"john\", \"k\", \"know\", \"l\", \"la\", \"le\", \"left\", \"let\", \"life\", \"like\", \"little\", \"long\", \"m\", \"made\", \"made\", \"make\", \"make\", \"man\", \"many\", \"may\", \"may\", \"men\", \"might\", \"mr\", \"much\", \"much\", \"must\", \"must\", \"n\", \"near\", \"never\", \"nine\", \"nothing\", \"o\", \"often\", \"one\", \"one\", \"p\", \"p\", \"part\", \"per\", \"place\", \"put\", \"q\", \"r\", \"s\", \"said\", \"said\", \"saw\", \"sect\", \"see\", \"self\", \"seven\", \"several\", \"shall\", \"shall\", \"sir\", \"six\", \"soon\", \"t\", \"take\", \"ten\", \"th\", \"thee\", \"therefore\", \"thing\", \"things\", \"thou\", \"though\", \"though\", \"three\", \"thus\", \"thy\", \"till\", \"time\", \"told\", \"took\", \"two\", \"two\", \"u\", \"u\", \"upon\", \"upon\", \"us\", \"v\", \"v\", \"vol\", \"w\", \"way\", \"well\", \"went\", \"whether\", \"without\", \"without\", \"would\", \"would\", \"x\", \"y\", \"yet\", \"z\"]\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usefulWordList = [word for word in query if word not in more_stopwords]\n",
    "query_lemmas = [wordnet_lemmatizer.lemmatize(word) for word in usefulWordList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assemble a Toy Corpus to Use for Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "textdirectory = home + '/dh2/corpora_and_metadata/chunked_files_kant/'\n",
    "\n",
    "os.chdir(textdirectory)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of filenames\n",
    "import glob\n",
    "filenames = glob.glob(\"*.txt\")\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in filenames:\n",
    "    with open (str(i),'r') as file:\n",
    "        readFile = file.read()\n",
    "        words = nltk.tokenize.word_tokenize(readFile)\n",
    "        usefulWordList = [word for word in words if word not in more_stopwords]\n",
    "        lemma_text = [wordnet_lemmatizer.lemmatize(word) for word in usefulWordList]\n",
    "        corpus.append(lemma_text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Dictionary of Word Counts for Your Toy Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = {}\n",
    "for i in range(len(filenames)):\n",
    "    corpus_dict[filenames[i]] = corpus[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "for i in corpus_dict:\n",
    "    for lemma in corpus_dict[i]:\n",
    "        if lemma not in word_freq.keys():\n",
    "            word_freq[lemma] = 1\n",
    "        else:\n",
    "            word_freq[lemma] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq[\"principle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma in sorted(word_freq, key=word_freq.get, reverse=True):\n",
    "    print(lemma, word_freq[lemma])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Derive TF-IDF Scores for Your Toy Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_idfs = {}\n",
    "for lemma in word_freq:\n",
    "    doc_containing_lemma = 0\n",
    "    for i in corpus_dict:\n",
    "       if lemma in corpus_dict[i]:\n",
    "        doc_containing_lemma += 1\n",
    "    lemma_idfs[lemma] = np.log(len(corpus)/(1 + doc_containing_lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemma_idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemma_idfs[\"principle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_corpus_tfidfs = {}\n",
    "for i in corpus_dict: \n",
    "    doc_word_counts = {}\n",
    "    doc_tfidfs = {}\n",
    "    for lemma in corpus_dict[i]:\n",
    "        if lemma not in doc_word_counts:\n",
    "            doc_word_counts[lemma] = 1\n",
    "        else:\n",
    "            doc_word_counts[lemma] += 1\n",
    "        doc_tfidfs[lemma] = doc_word_counts[lemma]*lemma_idfs[lemma]\n",
    "    complete_corpus_tfidfs[i] = doc_tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_corpus_tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_corpus_tfidfs[\"E000049.001.0000.txt\"][\"principle\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Finally, Write a Script to Perform Your Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = {}\n",
    "for i in corpus_dict:\n",
    "    doc_score = 0\n",
    "    for lemma in query_lemmas:\n",
    "        if lemma not in set(corpus_dict[i]):\n",
    "            pass\n",
    "        else:\n",
    "            doc_score += complete_corpus_tfidfs[i][lemma]\n",
    "        search_results[i] = doc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "search_results_df = pd.DataFrame(search_results, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results_df = search_results_df.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results_df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results_df = search_results_df.rename(columns={\"index\":\"ChunkName\", 0:\"agg_tfidf\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results_df.sort_values(by=['agg_tfidf'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"E000049.001.0105.txt\",'r') as result:\n",
    "    readResult = result.read()\n",
    "result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(readResult)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
