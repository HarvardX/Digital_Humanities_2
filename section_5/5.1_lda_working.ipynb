{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering Thematic Content with Latent Dirichlet Allocation - Working Notebook\n",
    "\n",
    "## 1. Preparing a Dataset for Analysis\n",
    "\n",
    "In this notebook, we're going to model the topics in a number of different sets of data by means of latent Dirichlet allocation (LDA). In effect, we'll be extracting the thematic DNA of whatever document or corpus we analyse.\n",
    "\n",
    "Like tagging, LDA is an automated method of text analysis, but you'll find that it differs from tagging in that we can intervene in how the process of topic modelling works by modifying the parameters with each run. This means, in short, that there is a great deal more to think about. It also means that as you change the parameters, your results will change, as well. What this means is that, whereas tagging strives for an objective, definitive description of the parts of speech or named entities in a text, the results we derive from topic modelling are much more provisional. This is compounded by the fact that topics are derived from a series of statistical inferences made by your computer, so that the results you get might vary, even as the parameters for a specific run remain unchanged.\n",
    "\n",
    "Let's again use our metadata to create a subcorpus. In this case, let's use the results from our search for `principle` tokens, which we produced in the simple counting lesson. To keep this relatively small we'll use only those texts that contain more than 500 occurences of the word. If you happen to find that this set is still too large for your computer to handle, continue to increase the principle count until you're working with only one or two volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell, change your working directory to `/dh2/corpora_and_metadata/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `glob` to get a list of `.csv` files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the big metadata set that contains the results for our `principles` search.\n",
    "ecco_metadata_w_principles = pd.read_csv(\"???\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take a look at the first ten rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecco_metadata_w_principles[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecco_metadata_w_principles = pd.read_csv(\"ecco_data_w_principles.csv\")\n",
    "filenames = ecco_metadata_w_principles.loc[(ecco_metadata_w_principles[\"principles\"] >= 100)][\"TCP\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_names = ecco_metadata_w_principles.loc[(ecco_metadata_w_principles[\"principles\"] >= 100)][\"Title\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(title_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see whether your `filenames` include the `.txt` extension. If they don't, run the following cells. Otherwise, you can skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_filenames_new = []\n",
    "for file in lda_filenames:\n",
    "    newstr = file + \".txt\"\n",
    "    lda_filenames_new.append(newstr)\n",
    "print(lda_filenames_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now import everything we'll need to perform our anaylsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation += \"“”‘’↩§†\"\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "more_stopwords = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '100', 'able', 'also', 'although', 'among', 'another', 'away', 'began', 'came', 'could', 'done', 'eight', 'even', 'ever', 'every', 'first', 'five', 'found', 'four', 'gave', 'give', 'go', 'however', 'indeed', 'left', 'like', 'made', 'make', 'many', 'may', 'might', 'much', 'must', 'near', 'never', 'nine', 'nothing', 'often', 'one', 'part', 'put', 'said', 'saw', 'see', 'seven', 'several', 'shall', 'six', 'soon', 'take', 'ten', 'thee', 'therefore', 'thing', 'things', 'thou', 'though', 'three', 'thy', 'till', 'time', 'told', 'took', 'two', 'upon', 'us', 'way', 'well', 'went', 'whether', 'without', 'would', 'yet', '’', '“', '”', ',', 'u']\n",
    "full_stopwords = (stopwords.words('english')) + more_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed earlier in this lesson, LDA works a lot better when it has many small texts to work with, rather than a few big ones. Let's divide our texts into smaller chunks of 1000 words each.\n",
    "\n",
    "First, we need to define a function to perform the text. Let's call it `text_splitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitter(filename, n_words):\n",
    "    with open(str(i), 'r') as file:\n",
    "        readFile = file.read()\n",
    "        tokenized_file = nltk.tokenize.word_tokenize(readFile)\n",
    "        file.close()\n",
    "        chunks = []\n",
    "        current_chunk_words = []\n",
    "        current_chunk_word_count = 0\n",
    "        for word in tokenized_file:\n",
    "            current_chunk_words.append(word)\n",
    "            current_chunk_word_count += 1\n",
    "            if current_chunk_word_count == n_words:\n",
    "                chunks.append(' '.join(current_chunk_words))\n",
    "                current_chunk_words = []\n",
    "                current_chunk_word_count = 0\n",
    "        chunks.append(' '.join(current_chunk_words))\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdirectory = home + '/dh2/corpora_and_metadata/sec5/'\n",
    "\n",
    "os.chdir(textdirectory)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_directory = os.path.join(textdirectory, r'chunked_files_principles')\n",
    "if not os.path.exists(new_directory):\n",
    "   os.makedirs(new_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run `text_splitter` and write each chunk to a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdirectory = home + '/dh2/corpora_and_metadata/working_set_cleaned/'\n",
    "\n",
    "os.chdir(textdirectory)\n",
    "print(os.getcwd())\n",
    "\n",
    "output_dir = home + '/dh2/corpora_and_metadata/unit5_files/chunked_files_principles/'\n",
    "\n",
    "chunk_length = 1000\n",
    "chunks = []\n",
    "for i in filenames_new:\n",
    "    chunk_counter = 0\n",
    "    texts = text_splitter(i, chunk_length)\n",
    "    for text in texts:\n",
    "        chunk = {'text': text, 'number': chunk_counter, 'filename': i}\n",
    "        chunks.append(chunk)\n",
    "        chunk_counter += 1\n",
    "    for chunk in chunks:\n",
    "        basename = os.path.basename(chunk['filename'])\n",
    "        basename = os.path.splitext(basename)[0]\n",
    "        fn = os.path.join(output_dir, \"{}.{:04d}.txt\".format(basename, chunk['number']))\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(chunk['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Starting LDA in Earnest: Creating a Dictionary and Corpus\n",
    "\n",
    "Now that we've created a directory of chunked files, we're ready to start our analysis. The first order of business is to read and process each of the chunks we just created, and then to process the files by tokenizing, removing stopword, and lemmatizing. Once that's done, we can create a common dictionary and a corpus from the entire set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdirectory = home + '/dh2/corpora_and_metadata/unit5_files/chunked_files_principles/'\n",
    "os.chdir(textdirectory)\n",
    "filenames = glob.glob(\"*.txt\")\n",
    "\n",
    "list_files = []\n",
    "for i in filenames:\n",
    "    with open (str(i),'r') as file:\n",
    "        readFile = file.read()\n",
    "        file.close()\n",
    "        chunks = []\n",
    "        current_chunk_words = []\n",
    "        current_chunk_word_count = 0\n",
    "        tokenized_file = nltk.tokenize.word_tokenize(readFile)\n",
    "        usefulTxt = [word for word in tokenized_file if word not in (full_stopwords)]\n",
    "        lemmas = [wordnet_lemmatizer.lemmatize(word) for word in usefulTxt]\n",
    "        list_files.append(lemmas)\n",
    "common_dictionary = Dictionary(list_files)\n",
    "common_corpus = [common_dictionary.doc2bow(file) for file in list_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the LDA Model\n",
    "\n",
    "At this point, we have two documents: `common_dictionary` and `common_corpus`. This is where things start to get interesting, as we set the training parameters that determine exactly how LDA will run.\n",
    "\n",
    "\n",
    "    <b>num_topics</b>: The number of topics to extract;\n",
    "    <b>chunksize</b>: The number of documents that will be used in training each chunk;\n",
    "    <b>passes</b>: The number of passes that the script makes through the process during training;\n",
    "    <b>iterations</b>: The maximum number of iterations through a corpus when inferring the topic distribution of that corpus;\n",
    "    <b>eval_every</b>: The number of passes after which the model's perplexity is evaluated. This can really slow down your script, so we'll generally leave it as 'None';\n",
    "    <b>alpha</b>: The higher your alpha, the more similar your topic contents;\n",
    "    <b>eta</b>: The same is true for beta. A higher value here results in more similar word contents;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10 # standard is 10\n",
    "chunksize = 200 # standard is 2000\n",
    "passes = 20 # standard is 20\n",
    "iterations = 400 # standard is 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "alpha = 0.1 #Originally set to 0.1\n",
    "eta = 0.01 #Originally set to 0.1\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = common_dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = common_dictionary.id2token\n",
    "model = LdaModel(\n",
    "    corpus=common_corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha=alpha, # 'auto' - Generally 50/T, where T is the number of Topics anticipated - 0.1 is standard\n",
    "    eta=eta, # 'auto' - Generally 200/W, where W is the number of words in the vocabulary - 0.1 is standard\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finally, run your LDA Script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(common_corpus, topn = 20) # num_words=20)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating Your Initial Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a close look at these results. Remember, as you try to grapple with the results, that this is a very small run, and the quality of your results will generally improve as you increase the size of your dataset.\n",
    "\n",
    "Allowing that these are provisional results, you may be able to see some obvious opportunities to improve our results, on a second pass. When you examine your results, ask yourself the following questions:\n",
    "\n",
    "    - Are the topics sufficiently coherent? That is, can I imagine how the words in this topic would tend to cluster around a specific subject or theme?;\n",
    "    - Are the topics sufficiently distinct? That is, do I see the same words coming up again and again, or am I getting clear differentiation from one group of words to the next?;\n",
    "    - Are there any obvious opportunities to improve the quality of my run by removing additional stopwords? This is an important question to ask, because often the most prevalent topics will be composed, in part, of words that don't have a great deal of semantic specificity. Might some of these be candidates to include in a subsequent list of stop words?\n",
    "    \n",
    "If you have concerns about these first two general problem areas, you might consider adjusting your `alpha` and `eta` parameters. If you notice new candidates for stop words, it's a simple matter to add these to your stop-word list and to rerun the analysis. Feel free to modify the other parameters, but let's focus here on stop words that it might be good to remove. Keep in mind that if you see a particular problem, like a stray letter or a numeral, you may as well try to remove a good number of that particular <i>type</i> of token, so that no tokens of the same sort will pollute your subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stopwords = [\"0\", \"1\", \"10\", \"100\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"a\", \"able\", \"adam\", \"also\", \"also\", \"although\", \"among\", \"another\", \"away\", \"b\", \"began\", \"c\", \"c\", \"came\", \"could\", \"d\", \"de\", \"done\", \"e\", \"eight\", \"et\", \"even\", \"even\", \"ever\", \"every\", \"every\", \"f\", \"first\", \"five\", \"found\", \"four\", \"g\", \"gave\", \"give\", \"go\", \"good\", \"great\", \"h\", \"high\", \"however\", \"i\", \"ii\", \"iii\", \"indeed\", \"j\", \"john\", \"k\", \"know\", \"l\", \"la\", \"le\", \"left\", \"let\", \"life\", \"like\", \"little\", \"long\", \"m\", \"made\", \"made\", \"make\", \"make\", \"man\", \"many\", \"may\", \"may\", \"men\", \"might\", \"mr\", \"much\", \"much\", \"must\", \"must\", \"n\", \"near\", \"never\", \"nine\", \"nothing\", \"o\", \"often\", \"one\", \"one\", \"p\", \"p\", \"part\", \"per\", \"place\", \"put\", \"q\", \"r\", \"s\", \"said\", \"said\", \"saw\", \"sect\", \"see\", \"self\", \"seven\", \"several\", \"shall\", \"shall\", \"sir\", \"six\", \"soon\", \"t\", \"take\", \"ten\", \"th\", \"thee\", \"therefore\", \"thing\", \"things\", \"thou\", \"though\", \"though\", \"three\", \"thus\", \"thy\", \"till\", \"time\", \"told\", \"took\", \"two\", \"two\", \"u\", \"u\", \"upon\", \"upon\", \"us\", \"v\", \"v\", \"vol\", \"w\", \"way\", \"well\", \"went\", \"whether\", \"without\", \"without\", \"would\", \"would\", \"x\", \"y\", \"yet\", \"z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_files = []\n",
    "for file in list_files:\n",
    "    file = [word for word in file if word not in (more_stopwords)]\n",
    "    reduced_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary = Dictionary(reduced_files)\n",
    "common_corpus = [common_dictionary.doc2bow(file) for file in reduced_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "num_topics = 10 # standard is 10\n",
    "chunksize = 200 # standard is 2000 - depends very much on the number of documents you have\n",
    "passes = 20 # standard is 20\n",
    "iterations = 400 # standard is 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "alpha = 0.1\n",
    "eta = 0.1\n",
    "\n",
    "temp = common_dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = common_dictionary.id2token\n",
    "model = LdaModel(\n",
    "    corpus=common_corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha=alpha, # 'auto' - Generally 50/T, where T is the number of Topics anticipated - 0.1 is good\n",
    "    eta=eta, # 'auto' - Generally 200/W, where W is the number of words in the vocabulary - 0.1 is good\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(common_corpus, topn = 20) # top_n=20, normally)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
