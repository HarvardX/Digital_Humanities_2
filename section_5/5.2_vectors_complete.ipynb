{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics of Word Embeddings\n",
    "\n",
    "A few short lessons ago, we introduced how to count the number of words in a document, which simply involved dividing that document up into a list of tokens and counting the length of that list. Getting a count is straightforward. Your computer is essentially doing the same sort of thing that you could perform, if you diligently sat down with a pen and paper and counted the number of words on every page of a physical text. It wouldn't be fun, but you could do it, and it's easy to conceptualize the sort of operation that's involved.\n",
    "\n",
    "Word embeddings, or, if you prefer, word vectors, are different. They're much harder to conceptualize, even though the basic operation, of mapping words onto numbers remains the same. The basic goal of word embeddings is to develop a model of your dataset in which the semantic relationships between the words is given a numerical representation. If that sounds abstract, it is. Essentially your computer uses a predefined method to analyse the way a word is distributed in your corpus. Over many passes, it attempt to \"learn\" the deep relationships among words, and to place similar words in similar contexts.\n",
    "\n",
    "You can think of it like this. Imagine that you have the tokens for `Newton`, `Leibniz`, and `Descartes`, three of the most important physicists of the seventeenth century. This is simplifying things a bit, but you can think of a word embedding like this: its goal is to create a model in which your computer assigns a huge number of mathematical relationships to each of these words and the words that tend to occur with them. The key is that the mathematical relationships between these different words and their contexts should be similar. So, for instance, the relationship between `Newton` and `English` should be similar to that of `Leibniz` and `German`, or `Descartes` and `French`. Now, imagine that all these words are similarly related to all the other words in the dataset, so that, for instance, `English` and `London` has a relationship that is mathematically similar to `French` and `Paris`, or, if you prefer, between `England` and `London` and `France` and `Paris`. These mathematical relationships, ultimately, create an enormous, multidimensional web of semantic interconnectedness, which allows us to study the deep relationships between any words, as they occur in the dataset.\n",
    "\n",
    "Again, this all sounds a little abstract, so let's produce an embedding model, and see what it can do. Before we do, though, there are a lot of little white lies in what I just said. If you want a more detailed explanation of what happens when your computer produces a model of word embeddings, you can read <a href = \"http://jalammar.github.io/illustrated-word2vec/\">this article,</a> which does a great job of using simple graphics to illustrate the concept.\n",
    "\n",
    "## 1. Initial Steps\n",
    "\n",
    "To get started, by creating a dictionary out of the files in our `/working_set_cleaned/` corpus. This is, admittedly, a rather large set (although not terribly large, by word-embedding standards), and the scripts in this notebook can take a fair time to run. If you find that your script is taking too long, stop it. Instead, you can feed in a smaller directory, like the '/unit5_files/chunked_files_principles/', which will load much more quickly, while still producing reasonable results in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "textdirectory = home + '/dh2/corpora_and_metadata/working_set_cleaned/'\n",
    "\n",
    "os.chdir(textdirectory)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of filenames\n",
    "import glob\n",
    "print(glob.glob(\"*.txt\"))\n",
    "filenames = glob.glob(\"*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll make a giant list of all the files in our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Skip down to Section 3 of this notebook, if you would prefer to make a model with stemmed files </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "list_files = []\n",
    "for i in filenames:\n",
    "    with open (str(i),'r') as file:\n",
    "        readFile = file.read()\n",
    "        tokenized_file = nltk.tokenize.word_tokenize(readFile)\n",
    "        list_files.append(tokenized_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That done, let's return to our folder for Section 5.textdirectory = home + '/dh2/corpora_and_metadata/working_set_cleaned/'\n",
    "\n",
    "os.chdir(textdirectory)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdirectory = home + '/dh2/sec5/'\n",
    "\n",
    "os.chdir(textdirectory)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then analyse those separate files to create a single dictionary of all the unique tokens in that list of files. For this lesson, we'll be using [Gensim's word2vec model](https://radimrehurek.com/gensim/models/word2vec.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(list_files)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Train Word2Vec model. Defaults result vector size = 100\n",
    "model = Word2Vec(list_files, min_count = 0, workers=cpu_count())\n",
    "\n",
    "# Save and Load Model\n",
    "model.save('newmodel')\n",
    "model = Word2Vec.load('newmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the fun really begins. We'll be concentrating on two simple operations with this model. In the first, we'll use words that exist in clear binary pairs like young/old, man/woman, french/english (perhaps), and long/short to add and subtract qualities from another word.\n",
    "\n",
    "The format of this sort of operation is a little counterintuitive, in that it splits words into `positive` and `negative` categories:\n",
    "\n",
    " `result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=15)`\n",
    " \n",
    " `print(result)`\n",
    " \n",
    " `queen`\n",
    "\n",
    "   Or, if you prefer:\n",
    "   \n",
    " `king - man + woman = queen`\n",
    " \n",
    "Place the word to be manipulated in the first position of the `positive` list, the word to be added alongside it, and the word to be subtracted in the `negative` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=15)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following examples, or create your own. You won't always get perfect results, of course, but it's interesting to explore the dataset, nonetheless.\n",
    "\n",
    "`newton - english + french =`\n",
    "\n",
    "`king - man + woman =`\n",
    "\n",
    "`husband - male + female = `\n",
    "\n",
    "`puppy - young + old =`\n",
    "\n",
    "`dog - puppy + cat = `\n",
    "\n",
    "`grandison - man + woman =`\n",
    "\n",
    "`voltaire - french + english =`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.most_similar(positive=['???', '???'], negative=['???'], topn=???)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform a simpler operation, by checking out the `most_similar` tokens that are associated with a word. This looks for words in your model that are closely associated with your word of interest.\n",
    "\n",
    "Attempt this for the token `principle` (We'll use the results as queeries for our concept-search algorithm in the next unit), and then test out these words and any others you might like: `literature`, `novel`, `wig`, `garter`, `saladin`, `damask`, `liberty`, `puppy`, `pirate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('principle', topn=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new model with stemmed tokens, rather than just the straight tokens from before. Will be interesting to compare. Make sure to change the variable and file name for the model, especially when saving, so you don't overwrite the old one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start here to make a model with stemmed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "list_stemmed_files = []\n",
    "for i in filenames:\n",
    "    with open (str(i),'r') as file:\n",
    "        readFile = file.read()\n",
    "        tokenized_file = nltk.tokenize.word_tokenize(readFile)\n",
    "        stemmed_file = [ps.stem(word) for word in tokenized_file]\n",
    "        list_stemmed_files.append(stemmed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "stemmed_dictionary = corpora.Dictionary(list_stemmed_files)\n",
    "\n",
    "print(stemmed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Train Word2Vec model. Defaults result vector size = 100\n",
    "stemmed_model = Word2Vec(list_stemmed_files, min_count = 0, workers=cpu_count())\n",
    "\n",
    "# Save and Load Model\n",
    "stemmed_model.save('newmodel_stemmed')\n",
    "stemmed_model = Word2Vec.load('newmodel_stemmed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
