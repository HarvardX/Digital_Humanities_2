{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics of Word Embeddings\n",
    "\n",
    "The basic goal of word embeddings is to develop a model of your dataset in which the semantic relationships between the words is given a numerical representation. If that sounds abstract, it is. Essentially your computer uses a predefined method to analyse the way a word is distributed in your corpus. Over many passes, it attempt to \"learn\" the deep relationships among words, and to place similar words in similar contexts.\n",
    "\n",
    "You can think of it like this. Imagine that you have the tokens for `Newton`, `Leibniz`, and `Descartes`, three of the most important physicists of the seventeenth century. This is simplifying things a bit, but you can think of a word embedding like this: its goal is to create a model in which your computer assigns a huge number of mathematical relationships to each of these words and the words that tend to occur with them. The key is that the mathematical relationships between these different words and their contexts should be similar. So, for instance, the relationship between `Newton` and `English` should be similar to that of `Leibniz` and `German`, or `Descartes` and `French`. Now, imagine that all these words are similarly related to all the other words in the dataset, so that, for instance, `English` and `London` has a relationship that is mathematically similar to `French` and `Paris`, or, if you prefer, between `England` and `London` and `France` and `Paris`. These mathematical relationships, ultimately, create an enormous, multidimensional web of semantic interconnectedness, which allows us to study the deep relationships between any words, as they occur in the dataset.\n",
    "\n",
    "## 0. Visualizing Word Vectors with a Toy Corpus\n",
    "\n",
    "In this first video, I'm going to very briefly try to show you what a word vector model \"looks\" like. Of course, that's an impossible thing to do, because true vector models do not exist in Euclidean space, but I'm essentially going to create a stripped down, low-dimension model. In doing so, I'm drawing in part on an excellent [tutorial by Jason Brownlee](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/). I'll move rather quickly through this segment, explaining as I go in the video, so I'll keep my commentary in the markdown to a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I create a list of sentences, import a few standard libraries like nltk and numpy, and do a bit of cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [\n",
    "     (\"Richardson wrote novels.\"),\n",
    "     (\"Richardson wrote fiction.\"), \n",
    "     (\"Richardson wrote books.\"), \n",
    "     (\"Fielding wrote novels. An example is Amelia.\"), \n",
    "     (\"Austen wrote novels.\"), \n",
    "     (\"Richardson is a novelist and Austen is a novelist.\"),\n",
    "     (\"Fielding and Austen both wrote novels.\"),\n",
    "     (\"A novelist writes novels.\"), \n",
    "     (\"A novelist writes fiction.\"),\n",
    "     (\"A novelist writes books.\"),\n",
    "     (\"Novels are books.\"),\n",
    "     (\"Novels are books of fiction.\"),\n",
    "     (\"Richardson is an example of a novelist.\"),\n",
    "     (\"Fielding is an example of a novelist.\"),\n",
    "     (\"Austen is an example of a novelist.\"),\n",
    "     (\"Richardson wrote Pamela.\"),\n",
    "     (\"Fielding wrote Amelia.\"),\n",
    "     (\"Austen wrote Emma.\"),\n",
    "     (\"Pamela and Amelia are novels.\"),\n",
    "     (\"Amelia is an example of a fiction.\"),\n",
    "     (\"Emma is a fiction.\"),\n",
    "     (\"Amelia and Emma are books.\"),\n",
    "     (\"Novels are fiction.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "\n",
    "cleaned_sentences = []\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    sentence_txt = ''.join(c for c in sentence if c not in punctuation)\n",
    "    sentence_txt = sentence_txt.lower()\n",
    "    sentence_words = nltk.tokenize.word_tokenize(sentence_txt)\n",
    "    cleaned_sentences.append(sentence_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I get Word2Vec from gensim, which I'll use to generate my toy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_model = Word2Vec(cleaned_sentences, min_count=1, size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I have that, I can examine my model, view the words that it contains, and print it as an array..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_words = list(toy_model.wv.vocab)\n",
    "print(toy_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toy_model.wv['novels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_array = toy_model.wv[toy_model.wv.vocab]\n",
    "print(toy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I can go ahead and view the results as a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(toy_array[:, 0], toy_array[:, 1])\n",
    "words = list(toy_model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(toy_array[i, 0], toy_array[i, 1]))\n",
    "    plt.axhline(0, color='dimgrey')\n",
    "    plt.axvline(0, color='dimgrey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see is all the words in my toy set, spread out in two dimensions. I encourage you to try regenerating this mini vector model a few times. You'll find that it changes radically every time, due to the small size of my corpus and the probabilistic nature of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Steps\n",
    "\n",
    "Alright. Let's get down to business. To get started, create a dictionary out of the files in our `/working_set_cleaned/` corpus. This is, admittedly, a rather large set (although not terribly large, by word-embedding standards), and the scripts in this notebook can take a fair time to run. If you find that your script is taking too long, stop it. Instead, you can feed in a smaller directory, like the '/sec5/chunked_files_principles/', which will load much more quickly, while still producing reasonable results in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "textdirectory = home + '/dh2/corpora_and_metadata/working_set_cleaned/'\n",
    "\n",
    "os.chdir(textdirectory)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of filenames\n",
    "import glob\n",
    "print(glob.glob(\"*.txt\"))\n",
    "filenames = glob.glob(\"*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll make a giant list of all the files in our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Skip down to Section 3 of this notebook, if you would prefer to make a model with stemmed files </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "list_files = []\n",
    "for i in filenames:\n",
    "    with open (str(i),'r') as file:\n",
    "        readFile = file.read()\n",
    "        tokenized_file = nltk.tokenize.word_tokenize(readFile)\n",
    "        list_files.append(tokenized_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That done, let's return to our folder for Section 5.textdirectory = home + '/dh2/corpora_and_metadata/working_set_cleaned/'\n",
    "\n",
    "os.chdir(textdirectory)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdirectory = home + '/dh2/sec5/'\n",
    "\n",
    "os.chdir(textdirectory)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll then analyse those separate files to create a single dictionary of all the unique tokens in that list of files. For this lesson, we'll be using [Gensim's word2vec model](https://radimrehurek.com/gensim/models/word2vec.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(list_files)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Train Word2Vec model. Defaults result vector size = 100\n",
    "model = Word2Vec(list_files, min_count = 0, workers=cpu_count())\n",
    "\n",
    "# Save and Load Model\n",
    "model.save('newmodel')\n",
    "model = Word2Vec.load('newmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the fun really begins. We'll be concentrating on two simple operations with this model. In the first, we'll use words that exist in clear binary pairs like young/old, man/woman, french/english (perhaps), and long/short to add and subtract qualities from another word.\n",
    "\n",
    "The format of this sort of operation is a little counterintuitive, in that it splits words into `positive` and `negative` categories:\n",
    "\n",
    " `result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=15)`\n",
    " \n",
    " `print(result)`\n",
    " \n",
    " `queen`\n",
    "\n",
    "   Or, if you prefer:\n",
    "   \n",
    " `king - man + woman = queen`\n",
    " \n",
    "Place the word to be manipulated in the first position of the `positive` list, the word to be added alongside it, and the word to be subtracted in the `negative` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=15)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following examples, or create your own. You won't always get perfect results, of course, but it's interesting to explore the dataset, nonetheless.\n",
    "\n",
    "`newton - english + french =`\n",
    "\n",
    "`king - man + woman =`\n",
    "\n",
    "`husband - male + female = `\n",
    "\n",
    "`puppy - young + old =`\n",
    "\n",
    "`dog - puppy + cat = `\n",
    "\n",
    "`grandison - man + woman =`\n",
    "\n",
    "`voltaire - french + english =`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.most_similar(positive=['???', '???'], negative=['???'], topn=???)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform a simpler operation, by checking out the `most_similar` tokens that are associated with a word. This looks for words in your model that are closely associated with your word of interest.\n",
    "\n",
    "Attempt this for the token `principle` (We'll use the results as queeries for our concept-search algorithm in the next unit), and then test out these words and any others you might like: `literature`, `novel`, `wig`, `garter`, `saladin`, `damask`, `liberty`, `puppy`, `pirate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('principle', topn=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new model with stemmed tokens, rather than just the straight tokens from before. Will be interesting to compare. Make sure to change the variable and file name for the model, especially when saving, so you don't overwrite the old one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start here to make a model with stemmed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "list_stemmed_files = []\n",
    "for i in filenames:\n",
    "    with open (str(i),'r') as file:\n",
    "        readFile = file.read()\n",
    "        tokenized_file = nltk.tokenize.word_tokenize(readFile)\n",
    "        stemmed_file = [ps.stem(word) for word in tokenized_file]\n",
    "        list_stemmed_files.append(stemmed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "stemmed_dictionary = corpora.Dictionary(list_stemmed_files)\n",
    "\n",
    "print(stemmed_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Train Word2Vec model. Defaults result vector size = 100\n",
    "stemmed_model = Word2Vec(list_stemmed_files, min_count = 0, workers=cpu_count())\n",
    "\n",
    "# Save and Load Model\n",
    "stemmed_model.save('newmodel_stemmed')\n",
    "stemmed_model = Word2Vec.load('newmodel_stemmed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
