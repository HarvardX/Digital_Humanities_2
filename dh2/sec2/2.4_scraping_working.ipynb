{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping - Collecting Data about Monsters - Working Notebook\n",
    "\n",
    "## 1. First Steps: Using Beautiful Soup to Scrape a Wikipedia Page\n",
    "\n",
    "We are going to scrape Wikipedia for information about various mythological creatures. The information we want exists in multiple layers, like a tree structure. We're starting from [this list](https://en.wikipedia.org/wiki/Lists_of_legendary_creatures). This is a top level page which is a list of lists, one for each letter (eg `List of legendary creatures (A)`). The `A` sublist contains a list of creatures, as well as each creature's cultural origin. We can go one level deeper to the creature itself for even more information.\n",
    "- Lists of legendary creatures\n",
    "    - List of legendary creatures (A)\n",
    "    - List of legendary creatures (B)\n",
    "        - Ba (Egyptian)\n",
    "        - Baba Yaga (Slavic)\n",
    "        - Backoo (Guyanese) ...\n",
    "    - List of legendary creatures (C) ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the list of lists and try to extract our sublists. We'll use the Requests library, as we did with APIs, as well as [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#), which is a library for extracting data from HTML and XML. We're going to use the `html5lib` parser with Beautiful Soup because it is pretty lenient and creates valid HTML; the downside is that it is slightly slower than some other parsers.\n",
    "\n",
    "So, first, let's import all the modules we need, including Beautiful Soup, which we'll simply call `bs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup as bs\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our modules, the following cell will scrape our list of lists of legendary creatures.\n",
    "\n",
    "As with our API queries, the line of script that uses `requests` to do the scraping is very straightforward: `R = requests.get(list_of_lists_url)`.\n",
    "\n",
    "Once we have that HTML content, we'll then parse it into a tree structure of Python objects using Beautiful Soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_lists_url = \"https://en.wikipedia.org/wiki/Lists_of_legendary_creatures\"\n",
    "R = requests.get(list_of_lists_url)\n",
    "soup = bs(R.content, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the massive blob of HTML that we've just scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a little unweildly. Let's instead see a \"pretty\" version of the HTML tree using the `soup.prettify()` method. Once you run the following cell, the HTML will be structured in a much more human-readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, instead, we print the actual raw content using `print(R.content)`, we can see that that it looks like one long string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(R.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we downloaded without any formatting. It's actually a sequence of <b>byte literals</b>. You can tell this because byte literals are prefixed by `b` when printed.\n",
    "\n",
    "So far, so good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Navigating HTML with Beautiful Soup\n",
    "\n",
    "There's a lot that you can do with the HTML data that you've just scraped, but to do so you have to navigate this mass of information effectively. Fortunately, Beautiful Soup includes a number of methods that are built for just this purpose. Most of these methods isolate specific HTML tags, like `<title>` or `<body>` or `<p>`. We'll look at a few here, but you can always [consult the documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) if you want to see a full list.\n",
    "\n",
    "First of all, we can isolate the text from the page without any HTML markup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks a little messy at the top of the page, but if you scroll down, you'll see that a good deal of the text is captured quite cleanly.\n",
    "\n",
    "Similarly, you can isolate the `title` element. By using the `title` method by itself, you'll retain the HTML tags for this element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if you prefer, you can remove those tags by isolating the string itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we get the first paragraph element, using `.p`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to modify the cell above to locate other common HTML elements, like `.head`, `.body`, `.div` and `.a`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, you'll want to search for a specific occurence of a tag using attributes such as a classname or id. You can use the `.find()` method to specify these attributes. Let's grab the Table of Contents from our soup. In a browser window, open up [the Wikipedia page]('https://en.wikipedia.org/wiki/Lists_of_legendary_creatures'). Right click on the table of contents and choose \"Inspect\" (in Chrome) or \"Inspect Element\" (Firefox). This will show the current state of the DOM and open to the point in the DOM or \"Document Object Model\" where you've clicked. You should see that there is a `<div>` with an `id` of `\"toc\"`. Let's select that with BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.find(\"div\", id=\"toc\")\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It still looks a bit complicated, but we've cleanly isolated the Table of Contents for this particular page.\n",
    "\n",
    "Now that we've isolated this one `div` tag, we can see all sorts of information about it, including its type, name, attributes, and id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tag))\n",
    "print(tag.name)\n",
    "print(tag.attrs) \n",
    "print(tag['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have just used `soup.find(id=\"toc\")` because HTML IDs should be unique, but sometimes you'll want to combine selector attributes as we did here. This shows that BeautifulSoup tags are their own Python class, and have names and attributes. Those attributes can be accessed just like you would access a dictionary property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compiling the Alphabetical Lists of Legendary Creatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen, in order to compile a master list of legendary creatures, we'll have to scrape each of the alphabetical lists sequentially: all the creatures that start with \"A\" (like, \"Anubis\"), all that start with \"B\" (like, \"Basilisk\"), etc. To do that, we'll need the corresponding URL for each of the alphebetical lists.\n",
    "\n",
    "Let's start by trying to get every instance of the `a` or `anchor` tag, which marks each link on our page. To do this, we'll use the `.find_all()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup.find_all('a')\n",
    "print(len(all_links))\n",
    "all_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, that's way too many links! <b>(You'll probably see a different number from what Cole sees in the video.)</b> We need to find just the hyperlinks for the alphabetical lists. Go ahead and try finding one of the links in the developer's console and seeing if there are any special attributes that might allow us to easily select it.\n",
    "\n",
    "To make it easy for you, we've included this little snippet of our pretified HTML, so you can see the beginning of the list of links that concerns us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Prettified HTML](alphabetical_lists_-_legendary_creatures.png \"Alphabetical Lists - Legendary Creatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it looks like there's no class for the links we want, nor is there an id or class on the containing list element (`<ul>`). There is an ID in the span of the header above the list element we want (`Alphabetical_lists`). We'll need to do some slightly finicky traversing of the tree to get the data: first, find the span; then go from the span to the header (`parent`); get the first member of the parent's `siblings`; and finally, get all of the links in that unordered list (`<li>` tags in the `<ul>`). You can do this using `.contents`, which returns a list, or `.children`, which returns an iterator.\n",
    "\n",
    "Let's find the `span` with the `id` \"Alphabetical_lists` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_header = soup.find(id=\"Alphabetical_lists\")\n",
    "print(tag_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, navigating through this soup of HTML can be a little tricky, but beautifulsoup fortunately has a number of methods that can be used for just this purpose:\n",
    "\n",
    "    `parent` - moves from one element in the HTML to another that contains it\n",
    "    `children` - which does the opposite, moves from an an element to those that it contains\n",
    "    `next_sibling` - moves between two elements on the same level\n",
    "    \n",
    "Follow along as Cole works his way through the HTML to find the elements we need.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_parent = tag_header.parent\n",
    "print(tag_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_nextsib = tag_parent.next_sibling\n",
    "print(tag_nextsib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Cole points out, `next_sibling` will sometimes give you empty blank space. Instead, use `find_next_sibling` to proceed right to the unordered list that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_ul = tag_parent.find_next_sibling()\n",
    "print(tag_ul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one last step. Now that we have this list, we have to be able to isolate each subdirectory, first by singling out one HTML link, and then using `.get(\"href\")` to extract the subdirectory URL. It works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link1 = tag_ul.find(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link1.get(\"href\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've isolated both our unordered list and the subdirectory URL within each link, you should be able to use what you know to produce a list of each of the complete alphabetical URLs.\n",
    "\n",
    "On your own, take a stab at writing a loop that 1) singles out the list element for each letter, 2) identifies each URL subdirectory, 3) appends that subdirectory to the main wikipedia URL , which is `\"https://en.wikipedia.org\"`, and 4) finally, make a list of each of these complete URLs.\n",
    "\n",
    "Here, we've given you a head start with some hints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs =                   # Create an empty list\n",
    "\n",
    "for  in :                 # Isolate each link using `.find_all`. Should take the form `for X in Y:`\n",
    "    url =      +          # Using `.get(\"href\")`, isolate each subdirectory URL and add it to the main wikipedia URL\n",
    "                          # Append the full URL to your list\n",
    "print(hrefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pause the video here and complete the loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scraping List Pages and Creating a Dictionary of Creatures\n",
    "\n",
    "At this point, you've learned how to scrape a webpage and parse its HTML to find the elements that you need. That was our main objective. For the sake of completeness, we'll show you how Cole went on to scrape a much more complete database, with all sorts of information about these legendary creatures. Much of this is pretty finicky, so we're going to move much more quickly, and we won't be walking through each step in the process.\n",
    "\n",
    "First, we're going to compile all the creatures into one source. Let's make a dictionary for that, using creature names as keys. Then we'll want to create a function that we can reuse to scrape each of the list pages for the creatures. There are several pages that have some irregular items without links or the dashes we're using to separate the creature names from their short descriptions.\n",
    "\n",
    "Here's the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_creatures = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the function Cole wrote to populate his dictionary. If this looks complicated, much of the code here is involved in cleaning up the names and short descriptions for each creature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_list_page(url):\n",
    "    \"\"\"\n",
    "    Scrapes a list of creature pages.\n",
    "    Returns a dictionary of creatures with names, titles (of links), links, short descriptions, and cultures.\n",
    "    \"\"\"\n",
    "    \n",
    "    creatures = {}\n",
    "    R = requests.get(url)\n",
    "    soup = bs(R.content, 'html5lib')\n",
    "    list_items = soup.find(\"div\", class_=\"mw-parser-output\").find_all(\"ul\")[1].find_all(\"li\")\n",
    "    for li in list_items:\n",
    "        if len(li.find_all(\"a\")) < 1: # There are a couple items without links\n",
    "            split_text = li.get_text().split(\"-\", 1)[0].strip()\n",
    "            name = split_text[0].strip()\n",
    "            desc = split_text[1].strip()\n",
    "            creatures[name] = {\n",
    "                \"name\": name,\n",
    "                \"short_description\": desc\n",
    "            }\n",
    "        else:\n",
    "            name = li.a.contents[0]\n",
    "            creatures[name] = {}\n",
    "            creatures[name][\"name\"] = name\n",
    "            creatures[name][\"title\"] = li.a['title']\n",
    "            creatures[name][\"link\"] = requests.compat.urljoin(\"https://en.wikipedia.org/wiki\", li.a['href'])\n",
    "            if \"-\" in li.get_text(): # There are a couple items where the dash isn't surrounded by spaces\n",
    "                creatures[name][\"short_description\"] = li.get_text().split(\"-\", 1)[1].strip()\n",
    "            if(len(li.find_all(\"a\")) > 1): # Couple of items without links\n",
    "                creatures[name][\"culture\"] = li.find_all(\"a\")[1].contents[0]\n",
    "                creatures[name][\"culture_title\"] = li.find_all(\"a\")[1]['title']\n",
    "                creatures[name][\"culture_href\"] = requests.compat.urljoin(\"https://en.wikipedia.org/wiki\", li.find_all(\"a\")[1]['href'])\n",
    "    return creatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this on one page of our `hrefs` list to make sure it works. If that looks good, then we can run it on all the subpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hrefs[0])\n",
    "all_creatures.update(scrape_list_page(hrefs[0]))\n",
    "all_creatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful. Now that we have that information, let's run a loop that collects data from each of the alphabetical URLs we collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for creature_list_url in hrefs:\n",
    "    creatures = scrape_list_page(creature_list_url)\n",
    "    all_creatures.update(creatures)\n",
    "print(len(all_creatures.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our complete list of creatures, it's a simple matter to print the information for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_creatures[\"Vampire\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Our Creatures\n",
    "\n",
    "Great! That's a lot of data. Let's export it all to a CSV in case we want to import it again later. We could use the `csv` library, but Pandas has a quick handy function for this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "creatures_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, creature in all_creatures.items():\n",
    "    creatures_list.append(creature)\n",
    "df = pd.DataFrame(creatures_list)\n",
    "df.to_csv(\"all_creatures.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've made our CSV and the job is done! Find the csv in your section 2 directory and check in out!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
